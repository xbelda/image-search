{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ebeae77-a8af-46b5-ab5d-487faa4336a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9428f50c-4491-420c-b801-5666174705d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xavi/projects/image-search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xavi/projects/image-search/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3848d2-1983-4fec-b20f-a26c272c0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from image_search.data import ImagesDataset, ConversionsDataset\n",
    "from image_search.train import temporal_train_test_split\n",
    "from image_search.model import QueryModel, ImageModel, LightningImageSearchSigLIP\n",
    "\n",
    "from image_search.metrics import hit_rate, mean_average_precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8237ef51-3b14-4e7c-a575-59b348a8deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "IMAGES_FOLDER = Path(\"./data/unsplash-research-dataset-lite-latest/photos/\")\n",
    "BASE_MODEL = \"google/siglip-base-patch16-224\"\n",
    "BATCH_SIZE = 2048\n",
    "NUM_WORKERS = 4\n",
    "SEED = 42\n",
    "\n",
    "CHECKPOINT_PATHS = {\n",
    "    \"VANILLA\": None,\n",
    "    \"FINE-TUNED\": \"./mlruns/168016125050379525/b4956e72f44f4df48b395cf338545014/checkpoints/epoch=0-step=60598.ckpt\"\n",
    "}\n",
    "\n",
    "MODEL_NAME = \"FINE-TUNED\"\n",
    "CHECKPOINT_PATH = CHECKPOINT_PATHS[MODEL_NAME]\n",
    "OUTPUT_PATH = Path(\"./results/\") / f\"{MODEL_NAME}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a27126-c7df-4729-9e33-3138fa4a10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0be5df1-53e8-444b-98fa-4b535a9db636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(BASE_MODEL).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee8b75e-047f-4609-b564-cb7b250848b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECKPOINT_PATH is not None:\n",
    "    # Note: Some of these keyword arguments should not be necessary after the latest changes in the code (but this would require retraining the model)\n",
    "    lightning_model = LightningImageSearchSigLIP.load_from_checkpoint(CHECKPOINT_PATH, model=model, lr=1e-4)\n",
    "else:\n",
    "    # In case we want to load the vanilla model directly \n",
    "    lightning_model = LightningImageSearchSigLIP(model=model, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67aa2ad4-4b39-4cef-9a06-8d597550cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = lightning_model.to(device=\"cuda\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2983a8-8fd8-4374-aa67-53f2f2ae70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = lightning_model.image_model\n",
    "query_model = lightning_model.query_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd6d3b1-2d32-4def-969d-f382854bec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable eval mode\n",
    "image_model = image_model.eval()\n",
    "query_model = query_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d7520e-d35b-4bf6-b609-1f7860d1809c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f0e4caba8c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312615fc-846d-41b1-8844-8f7460cf1663",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19f22a38-b1f2-492f-8a06-008145598090",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dataset = ImagesDataset(image_folder=IMAGES_FOLDER, processor=processor)\n",
    "images_dataloader = torch.utils.data.DataLoader(images_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104c833-ae98-4e10-9df5-593ccac72103",
   "metadata": {},
   "source": [
    "# Generate Index\n",
    "Following the [Pinecone guide](https://www.pinecone.io/learn/series/faiss/faiss-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eb00336-460e-4b83-a6eb-b3c4142532a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = model.vision_model.config.hidden_size\n",
    "# Note: Since this dataset contains a pretty small number of images (25k) we can just use brute-force and use a Flat Faiss index.\n",
    "# This way we also avoid having to \"train\" the retriever.\n",
    "# In a real scenario, we would try to optimize for faster retrieval with potentially millions of items\n",
    "index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8efe17-2353-492d-a609-2d81ebec11be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [01:37<00:00,  7.51s/it]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(images_dataloader):\n",
    "    pixel_values = batch[\"pixel_values\"].to(device=lightning_model.device, dtype=lightning_model.dtype)\n",
    "    \n",
    "    image_embeddings = image_model(pixel_values=pixel_values)\n",
    "    image_embeddings = image_embeddings.to(device=\"cpu\", dtype=torch.float32).detach().numpy()\n",
    "\n",
    "    ids = batch[\"id\"].to(\"cpu\").detach().numpy()\n",
    "    index.add(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3fd3b15-3c12-4354-aa74-e0b7b1d3f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid re-computing all indices, we will save the current index\n",
    "faiss.write_index(index, f\"indices/{MODEL_NAME}.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23722023-4459-4bad-a599-a268ea72f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(f\"indices/{MODEL_NAME}.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518e682-e96d-42a2-9726-b84280f15cda",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b98257-c5e5-46dc-8b2f-dbcaf1e14f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversions = load_and_preprocess_data()\n",
    "conversions = pd.read_parquet(\"./data/clean/conversions.parquet\")\n",
    "_, conversions_val = temporal_train_test_split(conversions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110d0766-91bb-4141-a9a9-604391f189fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "image_dataset = ImagesDataset(image_folder=IMAGES_FOLDER, processor=processor)\n",
    "dataset_val = ConversionsDataset(data=conversions_val, image_dataset=image_dataset, processor=processor)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4924bb8f-3470-4c93-b08f-5a6799077dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 947/947 [1:02:23<00:00,  3.95s/it]\n"
     ]
    }
   ],
   "source": [
    "true_ids = []\n",
    "predicted_ids = []\n",
    "\n",
    "for batch in tqdm(dataloader_val):\n",
    "    input_ids = batch[\"input_ids\"].to(lightning_model.device)\n",
    "    pixel_values = batch[\"pixel_values\"].to(device=lightning_model.device, dtype=lightning_model.dtype)\n",
    "\n",
    "    ids = batch[\"ids\"].unsqueeze(dim=0).numpy()\n",
    "\n",
    "    query_embedding = query_model(input_ids)\n",
    "    image_embedding = image_model(pixel_values)\n",
    "    \n",
    "    query_embedding = query_embedding.to(device=\"cpu\", dtype=torch.float32).detach().numpy()\n",
    "    # image_embedding = image_embedding.to(\"cpu\").detach().numpy()\n",
    "\n",
    "    distances, indices = index.search(query_embedding, k=25)\n",
    "\n",
    "    true_ids.append(ids)\n",
    "    predicted_ids.append(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc400c3f-8ac5-476e-ac19-4b464a3495d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ids = np.concatenate(true_ids, axis=1)\n",
    "predicted_ids = np.concatenate(predicted_ids, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104054ef-9c25-4c9c-a326-ea59c226e6e1",
   "metadata": {},
   "source": [
    "# METRICS\n",
    "- (N)DCG: https://arize.com/blog-course/ndcg/\n",
    "- (Mean) Average Precion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51600c5b-0054-4a2f-9756-7a9cd8f9489b",
   "metadata": {},
   "source": [
    "### Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0f7a30b-40f1-44f4-807d-7a254530ad5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mAP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144425</td>\n",
       "      <td>0.144425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.348244</td>\n",
       "      <td>0.217511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.463021</td>\n",
       "      <td>0.232716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.601751</td>\n",
       "      <td>0.241641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hit_rate       mAP\n",
       "k                     \n",
       "1   0.144425  0.144425\n",
       "5   0.348244  0.217511\n",
       "10  0.463021  0.232716\n",
       "25  0.601751  0.241641"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = [1, 5, 10, 25]\n",
    "\n",
    "hit_rate_score = hit_rate(true_ids, predicted_ids, k=k)\n",
    "map_score = mean_average_precision_at_k(true_ids=true_ids, predicted_ids=predicted_ids, k=k)\n",
    "\n",
    "scores = pd.DataFrame({\n",
    "    \"hit_rate\": hit_rate_score,\n",
    "    \"mAP\": map_score,\n",
    "})\n",
    "scores.index.name = \"k\"\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e96eb2e0-6408-4109-ae10-874373273359",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.to_csv(OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
